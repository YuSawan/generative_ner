# Model Configuration
model:
  model_name: 'meta-llama/Meta-Llama-3-8B-Instruct' # Hugging Face model
  model_max_length: 4096
  prev_path: null # Pretrained Model Path: Use None if no pretrained model is being used
  cache_dir: null
  checkpoint_path: null

# Dataset Configuration
dataset:
  train_file: 'conll2003/train.jsonl'
  validation_file: 'conll2003/validation.jsonl'
  test_file: 'conll2003/test.jsonl'
  language: 'en' # Language of the prompt 'en' for English, 'ja' for Japanese.
  format: 'collective' # Format of the prompt, 'collective' for collective prompt (e.g "extract 'person, organization'"), "individual" for individual prompts (e.g "extract 'person'"), "universal" for universal prompts ('UniversalNER')
  labels2names: # Mapping of labels to IDs
    'PER': 'person'
    'ORG': 'organization'
    'LOC': 'location'
    'MISC': 'miscellaneous'
  # System prompt for the model
    # E.g. 'A virtual assistant answers questions from a user based on the provided text.'
    # E.g. 'バーチャルアシスタントは、提供されたテキストに基づいてユーザーの質問に答えます。'
  system_prompt: null

# Dataloader
remove_unused_columns: false

# Training Parameters
num_train_epochs: 2
per_device_train_batch_size: 2
per_device_eval_batch_size: 4
gradient_accumulation_steps: 5
gradient_checkpointing: true
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
bf16: true # Use bfloat16 for training

# Optimizer
optim: "adamw_torch"
adam_beta1: 0.9
adam_beta2: 0.98
adam_epsilon: 1.e-6

# Learning Rate and weight decay Configuration
learning_rate: 1.e-5
weight_decay: 0.1
max_grad_norm: 0.3

# logging
log_level: 'info'
logging_strategy: 'steps'
logging_steps: 100
report_to: 'wandb'

# Save
save_strategy: 'epoch'
save_total_limit: 3 #maximum amount of checkpoints to save
save_steps: 5

# Evaluation
eval_strategy: 'epoch'
metric_for_best_model: 'loss'
load_best_model_at_end: true
eval_on_start: false
