--per_device_train_batch_size 8
--per_device_eval_batch_size 32
--num_train_epochs 2
--learning_rate 1e-5
--weight_decay 0.01
--optim adamw_hf
--adam_beta1 0.9
--adam_beta2 0.98
--adam_epsilon 1e-6
--bf16 True
--gradient_accumulation_steps 5
--gradient_checkpointing True
--lr_scheduler_type cosine
--warmup_ratio 0.1
--max_grad_norm 0.3
--logging_steps 5
--save_steps 5
--save_total_limit 3
--log_level info
--logging_strategy steps
--logging_steps 100
--save_strategy epoch
--eval_strategy epoch
--metric_for_best_model loss
--report_to wandb
